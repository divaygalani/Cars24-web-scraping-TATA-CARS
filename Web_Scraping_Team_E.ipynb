{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDb2M94DeLJn"
   },
   "source": [
    "# **WEBSCRAPING CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLKJA6HCUx7u",
    "outputId": "bc527c8a-537a-44de-a6bf-9876f9309481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\divay\\appdata\\roaming\\python\\python313\\site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\divay\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\divay\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkIoGbRXVXNd"
   },
   "source": [
    "# Mumbai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- 1. Setup and Load Page ---\n",
    "my_url = \"https://www.cars24.com/buy-used-tata-cars-mumbai/?sort=bestmatch&serveWarrantyCount=true&listingSource=Homepage_Filters\"\n",
    "print(\"Opening browser...\")\n",
    "driver = webdriver.Edge() # Or webdriver.Chrome(), webdriver.Firefox() etc.\n",
    "driver.get(my_url)\n",
    "driver.maximize_window() \n",
    "print(\"Page loaded. Waiting for initial content...\")\n",
    "\n",
    "# List to hold the dictionaries of car data\n",
    "car_data = [] \n",
    "\n",
    "# --- STABLE CONTAINER SELECTOR ---\n",
    "# We will try the 'data-testid' first as it's the most stable\n",
    "container_xpath = \"//div[@data-testid='car-listing']\" \n",
    "fallback_xpath = \"//a[contains(@class, 'carCardWrapper')]\" # Fallback if data-testid fails\n",
    "\n",
    "try:\n",
    "    # Wait for at least one car card container to appear initially\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, container_xpath))\n",
    "    )\n",
    "    print(\"Initial listings found using 'data-testid'. Starting to scroll...\")\n",
    "except TimeoutException:\n",
    "    print(\"'data-testid' timed out. Trying fallback selector (class*='carCardWrapper')...\")\n",
    "    try:\n",
    "        # --- FALLBACK SELECTOR ---\n",
    "        container_xpath = fallback_xpath # Switch to the fallback\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, container_xpath))\n",
    "        )\n",
    "        print(\"Initial listings found using fallback selector. Starting to scroll...\")\n",
    "    except TimeoutException:\n",
    "        print(\"Page timed out AGAIN. Neither selector worked. Site structure has likely changed.\")\n",
    "        driver.quit()\n",
    "        raise SystemExit(\"Could not load initial listings.\") # Stop notebook execution\n",
    "    # --- END FALLBACK ---\n",
    "\n",
    "# --- 2. Scroll to Bottom to Load All Cars ---\n",
    "print(\"Scrolling to the bottom...\")\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2.5) \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        print(\"Reached bottom of the page.\")\n",
    "        break \n",
    "    last_height = new_height\n",
    "    # print(f\"Scrolling... new height {new_height}\") \n",
    "\n",
    "# --- 3. Find ALL Containers and Extract Data Directly ---\n",
    "try:\n",
    "    # Find all containers using the XPath that successfully loaded\n",
    "    containers = driver.find_elements(By.XPATH, container_xpath)\n",
    "    print(f\"Found {len(containers)} total car listings after scrolling.\")\n",
    "    print(\"Extracting data directly using Selenium...\")\n",
    "\n",
    "    successful_extractions = 0\n",
    "    for i, container in enumerate(containers):\n",
    "        # Set defaults for each car\n",
    "        year, car_name, kms, fuel, transmission, price = 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A' # Added car_name\n",
    "        \n",
    "        try:\n",
    "            # Find internal elements within this container using stable data-testid XPaths\n",
    "            try:\n",
    "                name_el = container.find_element(By.XPATH, \".//h2[@data-testid='car-title']\")\n",
    "                full_name = name_el.text.strip()\n",
    "                if 'Tata' in full_name:\n",
    "                    year = full_name.split(' ')[0]\n",
    "                    car_name = full_name # <-- STORE THE FULL CAR NAME\n",
    "                else:\n",
    "                    # This filters out any ads or non-Tata results\n",
    "                    continue \n",
    "            except NoSuchElementException:\n",
    "                continue # Skip if no title, it's not a car card\n",
    "\n",
    "            # Try finding the rest of the data\n",
    "            try: kms_el = container.find_element(By.XPATH, \".//li[@data-testid='car-kms-driven']\"); kms = kms_el.text.strip()\n",
    "            except NoSuchElementException: pass\n",
    "            try: fuel_el = container.find_element(By.XPATH, \".//li[@data-testid='car-fuel-type']\"); fuel = fuel_el.text.strip()\n",
    "            except NoSuchElementException: pass\n",
    "            try: trans_el = container.find_element(By.XPATH, \".//li[@data-testid='car-transmission']\"); transmission = trans_el.text.strip()\n",
    "            except NoSuchElementException: pass\n",
    "            try: \n",
    "                price_el = container.find_element(By.XPATH, \".//strong[@data-testid='car-price']\")\n",
    "                price = price_el.text.strip()\n",
    "            except NoSuchElementException: \n",
    "                 continue # Skip if no price\n",
    "\n",
    "            # Append data dictionary to the list\n",
    "            car_data.append({\n",
    "                'Year of Manufacture': year,\n",
    "                'Car Name': car_name, # <-- ADDED CAR NAME\n",
    "                'Kilometers Driven': kms,\n",
    "                'Fuel Type': fuel,\n",
    "                'Transmission': transmission,\n",
    "                'Price': price\n",
    "            })\n",
    "            successful_extractions += 1 # Count successful ones\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch unexpected errors during parsing of a single container\n",
    "            print(f\"Unexpected error parsing container {i+1}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error finding car containers after scrolling: {e}\")\n",
    "finally:\n",
    "    # --- 4. Close the Browser ---\n",
    "    driver.quit()\n",
    "    print(\"Browser closed.\")\n",
    "\n",
    "print(f\"Finished processing. Extracted data for {successful_extractions} cars.\")\n",
    "\n",
    "# --- 5. Convert Data to DataFrame and Save ---\n",
    "print(\"\\n---------------------------------\")\n",
    "print(\"Excel Conversion Process:\")\n",
    "\n",
    "if car_data:\n",
    "    print(f\"Successfully parsed {len(car_data)} car listings. Converting to Excel...\")\n",
    "    \n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(car_data)\n",
    "    \n",
    "    # Ensure columns are in the desired order\n",
    "    # <-- ADDED 'Car Name' TO THE COLUMN LIST\n",
    "    df = df[['Year of Manufacture', 'Car Name', 'Kilometers Driven', 'Fuel Type', 'Transmission', 'Price']] \n",
    "    \n",
    "    # Save the DataFrame to an Excel file\n",
    "    excel_filename = 'cars24_tata_data.xlsx'\n",
    "    try:\n",
    "        df.to_excel(excel_filename, index=False, sheet_name='Tata Cars')\n",
    "        print(f\"Data successfully saved to {excel_filename}\")\n",
    "        print(\"\\nDataFrame Head (first 5 rows):\")\n",
    "        display(df.head()) # Use display() in Jupyter for pretty table output\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel file '{excel_filename}': {e}\")\n",
    "        print(\"Make sure the file is not open in Excel.\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid car data was captured. No Excel file was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6lr5UzZVcc1"
   },
   "source": [
    "# Delhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install google-colab-selenium\n",
    "import google_colab_selenium as gs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "driver = gs.Chrome()\n",
    "driver.get(\"https://www.cars24.com/buy-used-tata-cars-new-delhi/?sort=bestmatch&serveWarrantyCount=true&listingSource=Homepage_Filters\")\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "soup.prettify()\n",
    "soup.prettify()\n",
    "carNameType=[]\n",
    "cars=soup.find_all('div',class_=\"sc-fLVwEd hRljRx\")\n",
    "for car in cars:\n",
    "  carNameType.append({\n",
    "      'Name':car.find('span',class_='sc-braxZu kjFjan').get_text(strip=True),\n",
    "      \"Type\":car.find('span',class_='sc-braxZu lmmumg').get_text(strip=True)\n",
    "  })\n",
    "carNameType\n",
    "all_cars=[]\n",
    "containers = soup.find_all(\"ul\", class_=\"sc-huvEkS gkjlEH\")\n",
    "\n",
    "for container in containers:\n",
    "  all_cars.append(\n",
    "     container.find_all(\"p\", class_=\"sc-braxZu kvfdZL\")\n",
    "  )\n",
    "all_cars\n",
    "headings = ['Mileage', 'Fuel', 'Transmission', 'Registration']\n",
    "\n",
    "# Convert into list of dictionaries:\n",
    "structured_data = []\n",
    "for row in all_cars:\n",
    "    # Use zip to pair headings with row values\n",
    "    entry = { heading: value for heading, value in zip(headings, row) }\n",
    "    structured_data.append(entry)\n",
    "structured_data\n",
    "carPrice=[]\n",
    "Prices=soup.find_all('div',class_=\"styles_priceWrap__VwWBV\")\n",
    "for Price in Prices:\n",
    "  carPrice.append({\n",
    "      'Actual_Price':car.find('span',class_='sc-braxZu gbxhkm'),\n",
    "      \"Discounted_price\":car.find('span',class_='sc-braxZu cyPhJl')\n",
    "  })\n",
    "\n",
    "allPrice\n",
    "merged = []\n",
    "for info, details in zip(carNameType, structured_data):\n",
    "    combined = info.copy()        # start with name & type\n",
    "    # add all keys from details into this dict\n",
    "    for k, v in details.items():\n",
    "        # If v is a BeautifulSoup Tag, extract the text\n",
    "        text = v.get_text(strip=True) if hasattr(v, \"get_text\") else v\n",
    "        combined[k] = text\n",
    "    merged.append(combined)\n",
    "merged\n",
    "import csv\n",
    "\n",
    "def export_to_csv(records, filename=\"output.csv\"):\n",
    "    if not records:\n",
    "        print(\"No records to write.\")\n",
    "        return\n",
    "\n",
    "    # Extract column headers from the keys of the first dictionary\n",
    "    fieldnames = list(records[0].keys())\n",
    "\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()    # Write header row\n",
    "        writer.writerows(records)  # Write data rows\n",
    "\n",
    "    print(f\"Successfully wrote {len(records)} records to {filename}\")\n",
    "\n",
    "\n",
    "export_to_csv(merged, filename=\"Tata_Used_Car.csv\")\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# Bengaluru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install google-colab-selenium\n",
    "import google_colab_selenium as gs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "driver = gs.Chrome()\n",
    "driver.get(\"https://www.cars24.com/buy-used-tata-cars-banglore/?sort=bestmatch&serveWarrantyCount=true&listingSource=Homepage_Filters\")\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "soup.prettify()\n",
    "soup.prettify()\n",
    "carNameType=[]\n",
    "cars=soup.find_all('div',class_=\"sc-fLVwEd hRljRx\")\n",
    "for car in cars:\n",
    "  carNameType.append({\n",
    "      'Name':car.find('span',class_='sc-braxZu kjFjan').get_text(strip=True),\n",
    "      \"Type\":car.find('span',class_='sc-braxZu lmmumg').get_text(strip=True)\n",
    "  })\n",
    "carNameType\n",
    "all_cars=[]\n",
    "containers = soup.find_all(\"ul\", class_=\"sc-huvEkS gkjlEH\")\n",
    "\n",
    "for container in containers:\n",
    "  all_cars.append(\n",
    "     container.find_all(\"p\", class_=\"sc-braxZu kvfdZL\")\n",
    "  )\n",
    "all_cars\n",
    "headings = ['Mileage', 'Fuel', 'Transmission', 'Registration']\n",
    "\n",
    "# Convert into list of dictionaries:\n",
    "structured_data = []\n",
    "for row in all_cars:\n",
    "    # Use zip to pair headings with row values\n",
    "    entry = { heading: value for heading, value in zip(headings, row) }\n",
    "    structured_data.append(entry)\n",
    "structured_data\n",
    "carPrice=[]\n",
    "Prices=soup.find_all('div',class_=\"styles_priceWrap__VwWBV\")\n",
    "for Price in Prices:\n",
    "  carPrice.append({\n",
    "      'Actual_Price':car.find('span',class_='sc-braxZu gbxhkm'),\n",
    "      \"Discounted_price\":car.find('span',class_='sc-braxZu cyPhJl')\n",
    "  })\n",
    "\n",
    "allPrice\n",
    "merged = []\n",
    "for info, details in zip(carNameType, structured_data):\n",
    "    combined = info.copy()        # start with name & type\n",
    "    # add all keys from details into this dict\n",
    "    for k, v in details.items():\n",
    "        # If v is a BeautifulSoup Tag, extract the text\n",
    "        text = v.get_text(strip=True) if hasattr(v, \"get_text\") else v\n",
    "        combined[k] = text\n",
    "    merged.append(combined)\n",
    "merged\n",
    "import csv\n",
    "\n",
    "def export_to_csv(records, filename=\"output.csv\"):\n",
    "    if not records:\n",
    "        print(\"No records to write.\")\n",
    "        return\n",
    "\n",
    "    # Extract column headers from the keys of the first dictionary\n",
    "    fieldnames = list(records[0].keys())\n",
    "\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()    # Write header row\n",
    "        writer.writerows(records)  # Write data rows\n",
    "\n",
    "    print(f\"Successfully wrote {len(records)} records to {filename}\")\n",
    "\n",
    "\n",
    "export_to_csv(merged, filename=\"Tata_Used_Car.csv\")\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Centre          car_name  Year_of_Manufacture Kilometers_Driven Fuel_Type  \\\n",
      "0  Mumbai    2015 Tata Zest                 2015         40.97k km    Petrol   \n",
      "1  Mumbai   2024 Tata NEXON                 2024          8.33k km    Petrol   \n",
      "2  Mumbai   2023 Tata PUNCH                 2023         42.18k km    Petrol   \n",
      "3  Mumbai   2019 Tata TIGOR                 2019         39.65k km    Petrol   \n",
      "4  Mumbai  2024 Tata ALTROZ                 2024          2.93k km    Petrol   \n",
      "\n",
      "  Transmission      Price  \n",
      "0       Manual  2.45 lakh  \n",
      "1         Auto      9.86L  \n",
      "2       Manual      6.27L  \n",
      "3       Manual      4.24L  \n",
      "4       Manual      8.67L  \n",
      "Index(['Centre', 'car_name', 'Year_of_Manufacture', 'Kilometers_Driven',\n",
      "       'Fuel_Type', 'Transmission', 'Price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LOAD DATA\n",
    "df = pd.read_excel(\"Tata Cars 24.xlsx\")\n",
    "\n",
    "# QUICK CLEAN / INSPECTION\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# Centre-wise Fuel Type Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_count = df.groupby([\"centre\", \"fuel_type\"]).size().unstack()\n",
    "\n",
    "fuel_count.plot(kind='bar')\n",
    "plt.title(\"Centre wise Fuel Type Availability\")\n",
    "plt.xlabel(\"Centre\")\n",
    "plt.ylabel(\"Count of Cars\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# Centre-wise Transmission Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_count = df.groupby([\"centre\", \"transmission\"]).size().unstack()\n",
    "\n",
    "trans_count.plot(kind='bar')\n",
    "plt.title(\"Centre wise Transmission Availability\")\n",
    "plt.xlabel(\"Centre\")\n",
    "plt.ylabel(\"Count of Cars\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# Correlation Between Price & KM Driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[\"price\"].corr(df[\"kms_driven\"])\n",
    "print(\"Correlation between Price & Kms Driven:\", corr)\n",
    "\n",
    "plt.scatter(df[\"kms_driven\"], df[\"price\"])\n",
    "plt.title(\"Price vs Kilometres Driven\")\n",
    "plt.xlabel(\"Kilometres Driven\")\n",
    "plt.ylabel(\"Price (in ₹)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slZXSM5LVjzH"
   },
   "source": [
    "# Distribution of Price vs Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  # if allowed, else reply \"NO SEABORN\" then I'll rewrite purely in matplotlib\n",
    "\n",
    "sns.histplot(data=df, x=\"price\", hue=\"fuel_type\", bins=10)\n",
    "plt.title(\"Distribution of Price Among Various Fuel Types\")\n",
    "plt.xlabel(\"Price (in ₹ Lakh)\")\n",
    "plt.ylabel(\"Number of Cars\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
